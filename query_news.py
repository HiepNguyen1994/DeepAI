### Query káº¿t há»£p BM25 

import os
import json
import faiss
import numpy as np
import openai
import re
from rank_bm25 import BM25Okapi
from datetime import datetime
from datetime import datetime, timedelta
import streamlit as st

openai.api_key = st.secrets["api"]["openai_key"]




# === FAISS News Handler ===
# def get_faiss_news_answer(query_text: str, start_date=None, end_date=None):
#     # === Setup ===
#     base_folder = os.getcwd()
#     articles_folder = os.path.join(base_folder, "articles_by_week")
#     faiss_index_path = os.path.join(base_folder, "faiss_banking_news.bin")
#     index = faiss.read_index(faiss_index_path)

#     # === Tá»± Ä‘á»™ng táº¡o ngÃ y náº¿u chÆ°a truyá»n ===
#     if not start_date or not end_date:
#         today = datetime.today()
#         start_date = (today - timedelta(days=30)).strftime("%Y-%m-%d")
#         end_date = today.strftime("%Y-%m-%d")

#     start_dt = datetime.strptime(start_date, "%Y-%m-%d")
#     end_dt = datetime.strptime(end_date, "%Y-%m-%d")

#     doc_name = query_text.strip()
def get_faiss_news_answer(query_text: str, start_date=None, end_date=None):
    # === Setup ===
    base_folder = os.getcwd()
    # Gá»¡ articles_folder â€“ KHÃ”NG DÃ™NG THÆ¯ Má»¤C Ná»®A
    # articles_folder = os.path.join(base_folder, "articles_by_week")

    faiss_index_path = os.path.join(base_folder, "faiss_banking_news.bin")
    index = faiss.read_index(faiss_index_path)

    # === Tá»± Ä‘á»™ng táº¡o ngÃ y náº¿u chÆ°a truyá»n ===
    if not start_date or not end_date:
        today = datetime.today()
        start_date = (today - timedelta(days=30)).strftime("%Y-%m-%d")
        end_date = today.strftime("%Y-%m-%d")

    start_dt = datetime.strptime(start_date, "%Y-%m-%d")
    end_dt = datetime.strptime(end_date, "%Y-%m-%d")

    doc_name = query_text.strip()

    # === Load dá»¯ liá»‡u tá»« file articles (gá»™p sáºµn)
    articles_path = os.path.join(base_folder, "articles_2025-03-17.json")  # hoáº·c Ä‘á»•i tÃªn láº¡i thÃ nh articles.json
    with open(articles_path, "r", encoding="utf-8") as f:
        articles_data = json.load(f)

    # === Lá»c theo ngÃ y
    filtered_articles = {
        url: art for url, art in articles_data.items()
        if start_dt <= datetime.strptime(art["date"], "%Y-%m-%d") <= end_dt
    }

    # === Pháº§n xá»­ lÃ½ tiáº¿p (ná»‘i vÃ o pháº§n index.search, get_embedding, v.v.)
    return filtered_articles


    # === DÃ¹ng GPT Ä‘á»ƒ trÃ­ch keyword cho BM25 ===
    def extract_keywords_by_gpt(prompt):
        try:
            # Náº¿u truy váº¥n ngáº¯n, dÃ¹ng luÃ´n
            if len(prompt.strip().split()) <= 2:
                return [prompt.strip().lower()]
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "Báº¡n lÃ  há»‡ thá»‘ng phÃ¢n tÃ­ch tin tá»©c ngÃ¢n hÃ ng. HÃ£y trÃ­ch ra cÃ¡c tá»« khÃ³a chÃ­nh xÃ¡c tá»« cÃ¢u há»i ngÆ°á»i dÃ¹ng Ä‘á»ƒ dÃ¹ng cho tÃ¬m kiáº¿m tin tá»©c (dáº¡ng BM25), chá»‰ giá»¯ láº¡i 3â€“5 cá»¥m tá»« quan trá»ng nháº¥t. Tráº£ vá» dáº¡ng list Python."},
                    {"role": "user", "content": f"Truy váº¥n: {prompt}"}
                ],
                max_tokens=100,
                temperature=0.2
            )
            raw = response.choices[0].message.content.strip()
            print(f"[DEBUG] GPT tráº£ vá»: {raw}")
            keywords = eval(raw)
            if not isinstance(keywords, list) or not keywords:
                raise ValueError("Invalid keyword list")
            return keywords
        except Exception as e:
            print(f"[ERROR] KhÃ´ng parse Ä‘Æ°á»£c keyword: {e}")
            return [prompt.strip().lower()]  # fallback vá» nguyÃªn cÃ¢u

    keyword = extract_keywords_by_gpt(query_text)

    # === Load táº¥t cáº£ file JSON ===
    all_json_files = [
        os.path.join(articles_folder, week_folder, f)
        for week_folder in os.listdir(articles_folder)
        if os.path.isdir(os.path.join(articles_folder, week_folder))
        for f in os.listdir(os.path.join(articles_folder, week_folder))
        if f.endswith(".json")
    ]

    if not all_json_files:
        return "âŒ KhÃ´ng tÃ¬m tháº¥y file tin tá»©c nÃ o."

    def extract_date(article):
        try:
            return datetime.strptime(article["date"], "%Y-%m-%d")
        except:
            return datetime.min

    filtered_documents = {}
    documents_for_bm25 = []
    article_ids = []

    for json_file in all_json_files:
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        for url, content in data.items():
            article_date = extract_date(content)
            if start_dt <= article_date <= end_dt:
                text = content.get("title", "") + " " + content.get("content", "")
                documents_for_bm25.append(text.split())
                article_ids.append(url)
                filtered_documents[url] = content

    if not documents_for_bm25:
        return "âŒ KhÃ´ng cÃ³ bÃ i bÃ¡o nÃ o trong khoáº£ng thá»i gian Ä‘Ã£ chá»n."

    bm25 = BM25Okapi(documents_for_bm25)
    bm25_scores = bm25.get_scores(" ".join(keyword).split())
    top_n = 200
    bm25_results = sorted(zip(article_ids, bm25_scores), key=lambda x: x[1], reverse=True)[:top_n]

    final_bm25_results = []
    for url, score in bm25_results:
        content = filtered_documents[url]["content"]
        if any(re.search(r"\b" + re.escape(kw) + r"\b", content, re.IGNORECASE) for kw in keyword):
            final_bm25_results.append((url, score))

    if not final_bm25_results:
        return f"âŒ KhÃ´ng cÃ³ bÃ i nÃ o chá»©a tá»« khÃ³a: {keyword}"

    filtered_documents = {url: filtered_documents[url] for url, score in final_bm25_results}
    bm25_article_ids = list(filtered_documents.keys())

    def search_faiss_news(keywords, bm25_article_ids, k=100):
        query_vector = np.array(
            openai.embeddings.create(model="text-embedding-3-large", input=" ".join(keywords)).data[0].embedding,
            dtype=np.float32
        ).reshape(1, -1)
        D, I = index.search(query_vector, k=k)

        results = []
        for idx in I[0]:
            if idx >= len(filtered_documents):
                continue
            article_url = list(filtered_documents.keys())[idx]
            if article_url in bm25_article_ids:
                article = filtered_documents[article_url]
                results.append({
                    "url": article_url,
                    "content": article["content"],
                    "date": article["date"]
                })
        return sorted(results, key=lambda x: extract_date(x), reverse=True)[:20]

    faiss_results = search_faiss_news(doc_name, bm25_article_ids, k=100)
    # faiss_results = search_faiss_news(keyword, bm25_article_ids, k=70)
    if not faiss_results:
        faiss_results = [
            {"url": url, "content": filtered_documents[url]["content"], "date": filtered_documents[url]["date"]}
            for url in bm25_article_ids[:5]
        ]

    context = "\n\n".join([
        f"ğŸ“Œ [{res['date']}] {res['url']}\n{res['content'][:10000]}..."
        for res in faiss_results
    ])

    response = openai.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "system",
                "content": (
                    "Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch ngÃ¢n hÃ ng, phÃ¢n tÃ­ch chi tiáº¿t nháº¥t cÃ³ thá»ƒ, dÃ i khoáº£ng 500 chá»¯. "
                    "HÃ£y tráº£ lá»i Ä‘Ãºng vÃ o truy váº¥n cá»§a ngÆ°á»i dÃ¹ng bÃªn dÆ°á»›i, chá»‰ dá»±a trÃªn ná»™i dung Ä‘Æ°á»£c cung cáº¥p. "
                    "Náº¿u khÃ´ng Ä‘á»§ dá»¯ liá»‡u, hÃ£y nÃ³i rÃµ thay vÃ¬ bá»‹a Ä‘áº·t. CÃ¢u tráº£ lá»i nÃªn logic, cÃ³ thá»ƒ trÃ­ch dáº«n má»‘c thá»i gian hoáº·c sá»‘ liá»‡u náº¿u cÃ³."
                ),
            },
            {
                "role": "user",
                "content": f"""ğŸ“Œ **Truy váº¥n:** {query_text.strip()}

ğŸ“– **Dá»¯ liá»‡u thu tháº­p Ä‘Æ°á»£c (tin tá»©c gáº§n Ä‘Ã¢y):**

{context}
"""
            },
        ],
        max_tokens=4000,
    )

    return response.choices[0].message.content





